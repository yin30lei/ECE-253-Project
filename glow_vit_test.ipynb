{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 23:24:56.660232: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-12 23:24:56.703114: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-12 23:24:56.703152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-12 23:24:56.704405: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 23:24:56.712557: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer, DefaultDataCollator\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "class GlowViT(ViTForImageClassification):\n",
    "    def help():\n",
    "        print(ViTForImageClassification.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yalu_ds_list = [(\"SeaSponge/wildme10_classify\", \"glow-vit\"),\n",
    "                (\"yin30lei/wildlife_very_dark\", \"glow-vit-dark\"),\n",
    "                (\"yin30lei/wildlife_well_illuminated\", \"glow-vit-illuminate\"),\n",
    "                (\"yin30lei/wildlife_mixed\", \"glow-vit-mix\")]\n",
    "\n",
    "# yalu_ds = yalu_ds_list[1][0]\n",
    "yalu_model_name = yalu_ds_list[3][1]\n",
    "# wildlife_test_ds = load_dataset(yalu_ds, cache_dir=Path.cwd() / \"yalu_dataset\", split=\"test\" , num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 labels for wildme10_classify\n",
    "num_labels = 11\n",
    "# print(wildlife_test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"lion\": 0, \"raccoon\": 1, \"tiger\": 2, \"wolf\": 3,\n",
    "            \"bear\": 4, \"hare\": 5, \"fox\": 6, \"deer\": 7, \n",
    "            \"leopard\" : 8, \"hyena\": 9, \"antelope\": 10}\n",
    "\n",
    "\n",
    "id2label = {v:k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yalu_checkpoint = f\"SeaSponge/{yalu_model_name}\"\n",
    "model = GlowViT.from_pretrained(yalu_checkpoint,\n",
    "                cache_dir=yalu_model_name,\n",
    "                label2id=label2id,\n",
    "                id2label=id2label,\n",
    "                num_labels=num_labels,\n",
    "                attn_implementation=\"sdpa\") # no flash attention yet for ViT model\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(yalu_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ds(examples):\n",
    "    images, labels = [], []\n",
    "    for image, label in zip(examples[\"image\"], examples[\"labels\"]):\n",
    "        pix_val = image_processor(images=image.convert(\"RGB\"), return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        pix_val.to(device)\n",
    "        #! supposed to be a number here\n",
    "        label = label2id[label]\n",
    "        images.append(pix_val)\n",
    "        labels.append(label)\n",
    "\n",
    "    return {\"pixel_values\": images, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Define the testing arguments\n",
    "\n",
    "testing_args = TrainingArguments(\n",
    "    output_dir=yalu_model_name,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=30,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=testing_args,\n",
    "    # data_collator=data_collator,\n",
    "    # eval_dataset=test_ds,\n",
    "    tokenizer=image_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_ds = [\"SeaSponge/wildme10_classify\", \"yin30lei/wildlife_very_dark\",\n",
    "#                  \"yin30lei/wildlife_well_illuminated\", \"yin30lei/wildlife_mixed\"]\n",
    "\n",
    "# for ds in main_ds:\n",
    "#     main_test_ds = load_dataset(ds, cache_dir=Path.cwd() / \"yalu_dataset\", split=\"test\" , num_proc=2)\n",
    "#     test_ds = main_test_ds.with_transform(transform_ds)\n",
    "    \n",
    "#     metrics = trainer.evaluate(test_ds)\n",
    "#     trainer.log_metrics(f\"{yalu_model_name} | testing on {ds} \", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `tensorboard --logdir <logging directory>` to get more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional_ds = [\"yin30lei/wildlife_very_dark_test\", \"yin30lei/wildlife_grayscale\",\n",
    "#                  \"yin30lei/wildlife_less_saturated\", \"yin30lei/wildlife_underexposed\"]\n",
    "# for ds in additional_ds:\n",
    "#     additional_test_ds = load_dataset(ds, cache_dir=Path.cwd() / \"yalu_dataset\", split=\"train\" , num_proc=2)\n",
    "#     test_ds = additional_test_ds.with_transform(transform_ds)\n",
    "    \n",
    "#     metrics = trainer.evaluate(test_ds)\n",
    "#     trainer.log_metrics(f\"{yalu_model_name} | testing on {ds} \", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verydark_filter = [\"yin30lei/wildlife_verydark_clahe\", \"yin30lei/wildlife_verydark_he\",\n",
    "#                  \"yin30lei/wildlife_verydark_afifi\"]\n",
    "# for ds in verydark_filter:\n",
    "#     additional_test_ds = load_dataset(ds, cache_dir=Path.cwd() / \"yalu_dataset\", split=\"train\" , num_proc=2)\n",
    "#     test_ds = additional_test_ds.with_transform(transform_ds)\n",
    "    \n",
    "#     metrics = trainer.evaluate(test_ds)\n",
    "#     trainer.log_metrics(f\"{yalu_model_name} | testing on {ds} \", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# underexposed_filter = [\"yin30lei/wildlife_underexposed_clahe\", \"yin30lei/wildlife_underexposed_he\",\n",
    "#                  \"yin30lei/wildlife_underexposed_afifi\"]\n",
    "# for ds in underexposed_filter:\n",
    "#     additional_test_ds = load_dataset(ds, cache_dir=Path.cwd() / \"yalu_dataset\", split=\"train\" , num_proc=2)\n",
    "#     test_ds = additional_test_ds.with_transform(transform_ds)\n",
    "    \n",
    "#     metrics = trainer.evaluate(test_ds)\n",
    "#     trainer.log_metrics(f\"{yalu_model_name} | testing on {ds} \", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 02:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** glow-vit-mix | testing on yin30lei/wildlife_less_saturated_clahe  metrics *****\n",
      "  eval_loss                   =      0.207\n",
      "  eval_model_preparation_time =     0.0029\n",
      "  eval_runtime                = 0:00:59.91\n",
      "  eval_samples_per_second     =     18.978\n",
      "  eval_steps_per_second       =      1.202\n",
      "***** glow-vit-mix | testing on yin30lei/wildlife_less_saturated_he  metrics *****\n",
      "  eval_loss                   =     0.1004\n",
      "  eval_model_preparation_time =     0.0029\n",
      "  eval_runtime                = 0:00:52.45\n",
      "  eval_samples_per_second     =     21.674\n",
      "  eval_steps_per_second       =      1.373\n",
      "***** glow-vit-mix | testing on yin30lei/wildlife_less_saturated_afifi  metrics *****\n",
      "  eval_loss                   =     0.0977\n",
      "  eval_model_preparation_time =     0.0029\n",
      "  eval_runtime                = 0:00:52.51\n",
      "  eval_samples_per_second     =     21.653\n",
      "  eval_steps_per_second       =      1.371\n"
     ]
    }
   ],
   "source": [
    "less_saturated_filter = [\"yin30lei/wildlife_less_saturated_clahe\", \"yin30lei/wildlife_less_saturated_he\",\n",
    "                 \"yin30lei/wildlife_less_saturated_afifi\"]\n",
    "for ds in less_saturated_filter:\n",
    "    additional_test_ds = load_dataset(ds, cache_dir=Path.cwd() / \"yalu_dataset\", split=\"train\" , num_proc=2)\n",
    "    test_ds = additional_test_ds.with_transform(transform_ds)\n",
    "    \n",
    "    metrics = trainer.evaluate(test_ds)\n",
    "    trainer.log_metrics(f\"{yalu_model_name} | testing on {ds} \", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

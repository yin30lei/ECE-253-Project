{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training\n",
    "\n",
    "For our project, we constructed a Vision Transformer (ViT) instance that we named `GlowViT` to assess how light exposure affects Neural Network's result at classification.\n",
    "\n",
    "> Note: This script trains the GlowViT model based on different processed datsets, thus the models have different names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 17:22:58.281792: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-11 17:22:58.325081: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-11 17:22:58.325118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-11 17:22:58.326413: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-11 17:22:58.334376: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import ast\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import albumentations\n",
    "from math import floor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer, DefaultDataCollator\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlowViT(ViTForImageClassification):\n",
    "    def help():\n",
    "        print(ViTForImageClassification.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f935961442cf4f198a3504019a7f45ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfb7b06b8a349ed8e67731d4f79c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9071dbe1ba41492da5e0bfaf04edcc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wild_train_ds = load_dataset(\"yin30lei/wildlife-from-wildme\", split=\"train[30:60%]\", cache_dir=Path.cwd() / \"wildlife\", num_proc=2)\n",
    "yalu_ds_list = [(\"SeaSponge/wildme10_classify\", \"glow-vit\"),\n",
    "                (\"yin30lei/wildlife_very_dark\", \"glow-vit-dark\"),\n",
    "                (\"yin30lei/wildlife_well_illuminated\", \"glow-vit-illuminate\"),\n",
    "                (\"yin30lei/wildlife_mixed\", \"glow-vit-mix\")]\n",
    "yalu_ds = yalu_ds_list[2][0]\n",
    "yalu_model_name = yalu_ds_list[2][1]\n",
    "wildlife_ds = load_dataset(yalu_ds, cache_dir=Path.cwd() / \"yalu_dataset\", num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildlife_train_ds = wildlife_ds[\"train\"]\n",
    "wildlife_test_ds = wildlife_ds[\"test\"]\n",
    "del wildlife_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': '60000760.jpg', 'image_id': 4964, 'width': 1024, 'height': 686, 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x686 at 0x7FDA192FA410>, 'labels': 'raccoon'}\n"
     ]
    }
   ],
   "source": [
    "# 11 labels for wildme10_classify\n",
    "num_labels = 11\n",
    "print(wildlife_train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_examples(ds, seed: int = 1234, examples_per_class: int = 3, size=(350, 350)):\n",
    "\n",
    "#     w, h = size\n",
    "#     print(f\"ds.features -> {ds.features}\")\n",
    "#     labels = ds.features['objects']['label']\n",
    "#     if not isinstance(labels, list):\n",
    "#         labels = [labels]\n",
    "#     grid = Image.new('RGB', size=(examples_per_class * w, len(labels) * h))\n",
    "#     draw = ImageDraw.Draw(grid)\n",
    "#     font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf\", 24)\n",
    "\n",
    "#     for label_id, label in enumerate(labels):\n",
    "\n",
    "#         # Filter the dataset by a single label, shuffle it, and grab a few samples\n",
    "#         ds_slice = ds.filter(lambda ex: ex['labels'] == label_id).shuffle(seed).select(range(examples_per_class))\n",
    "\n",
    "#         # Plot this label's examples along a row\n",
    "#         for i, example in enumerate(ds_slice):\n",
    "#             image = example['image']\n",
    "#             idx = examples_per_class * label_id + i\n",
    "#             box = (idx % examples_per_class * w, idx // examples_per_class * h)\n",
    "#             grid.paste(image.resize(size), box=box)\n",
    "#             draw.text(box, label, (255, 255, 255), font=font)\n",
    "\n",
    "#     return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_examples(wildlife_val_ds, seed=random.randint(0, 1337), examples_per_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"lion\": 0, \"raccoon\": 1, \"tiger\": 2, \"wolf\": 3,\n",
    "            \"bear\": 4, \"hare\": 5, \"fox\": 6, \"deer\": 7, \n",
    "            \"leopard\" : 8, \"hyena\": 9, \"antelope\": 10}\n",
    "\n",
    "\n",
    "id2label = {v:k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GlowViT were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "model = GlowViT.from_pretrained(checkpoint,\n",
    "                cache_dir=\"default_vit\",\n",
    "                label2id=label2id,\n",
    "                id2label=id2label,\n",
    "                num_labels=num_labels,\n",
    "                attn_implementation=\"sdpa\") # no flash attention yet for ViT model\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ds(examples):\n",
    "    images, labels = [], []\n",
    "    for image, label in zip(examples[\"image\"], examples[\"labels\"]):\n",
    "        pix_val = image_processor(images=image.convert(\"RGB\"), return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        pix_val.to(device)\n",
    "        #! supposed to be a number here\n",
    "        label = label2id[label]\n",
    "        images.append(pix_val)\n",
    "        labels.append(label)\n",
    "\n",
    "    return {\"pixel_values\": images, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildlife_train_ds_transformed = wildlife_train_ds.with_transform(transform_ds)\n",
    "test_ds = wildlife_test_ds.with_transform(transform_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[ 0.8353,  0.6784,  0.5059,  ..., -0.5137, -0.4980, -0.4745],\n",
      "         [ 0.8824,  0.7255,  0.5059,  ..., -0.5059, -0.4902, -0.4667],\n",
      "         [ 0.8980,  0.7569,  0.5451,  ..., -0.5059, -0.4824, -0.4667],\n",
      "         ...,\n",
      "         [ 0.9216,  0.9216,  0.9216,  ..., -0.1451, -0.1529, -0.2000],\n",
      "         [ 0.9529,  0.9529,  0.9529,  ..., -0.2314, -0.2627, -0.2941],\n",
      "         [-0.2471, -0.2471, -0.2471,  ..., -0.7020, -0.7098, -0.7176]],\n",
      "\n",
      "        [[ 0.7961,  0.6392,  0.4667,  ..., -0.5059, -0.4824, -0.4588],\n",
      "         [ 0.8431,  0.6863,  0.4667,  ..., -0.4980, -0.4745, -0.4510],\n",
      "         [ 0.8588,  0.7176,  0.5059,  ..., -0.4902, -0.4667, -0.4510],\n",
      "         ...,\n",
      "         [ 0.9294,  0.9294,  0.9294,  ..., -0.1137, -0.1216, -0.1686],\n",
      "         [ 0.9608,  0.9608,  0.9608,  ..., -0.2000, -0.2314, -0.2627],\n",
      "         [-0.2392, -0.2392, -0.2392,  ..., -0.6706, -0.6784, -0.6941]],\n",
      "\n",
      "        [[ 0.6471,  0.4902,  0.3098,  ..., -0.5765, -0.5686, -0.5686],\n",
      "         [ 0.6941,  0.5373,  0.3020,  ..., -0.5686, -0.5608, -0.5529],\n",
      "         [ 0.7098,  0.5686,  0.3412,  ..., -0.5686, -0.5608, -0.5608],\n",
      "         ...,\n",
      "         [ 0.8902,  0.8902,  0.8902,  ..., -0.1216, -0.1294, -0.1765],\n",
      "         [ 0.9216,  0.9216,  0.9216,  ..., -0.2078, -0.2392, -0.2706],\n",
      "         [-0.2784, -0.2784, -0.2784,  ..., -0.6784, -0.6863, -0.7020]]]), 'labels': 4}\n"
     ]
    }
   ],
   "source": [
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1090' max='1090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1090/1090 1:07:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         10.0\n",
      "  total_flos               = 3927088586GF\n",
      "  train_loss               =       0.0819\n",
      "  train_runtime            =   1:07:38.30\n",
      "  train_samples_per_second =       13.407\n",
      "  train_steps_per_second   =        0.269\n"
     ]
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "\n",
    "# It doesn't utilize GPU effectively, so trying some larger ViT models\n",
    "# Update: ViT large takes around 1GB, not very ideal. Some going to try other things\n",
    "\n",
    "#! with ViT-base, can afford to have large batchsizes\n",
    "# batchsz=2 -> 2133MiB / 11264MiB\n",
    "# running with larger batchsize actually increase training time, trying to find optimal largest batch\n",
    "# Update: This seems to go away when I put larger epoch numbers\n",
    "# When epoch=[10,20] -> 5, 6, 16, 32 (6117MiB / 11264MiB), 50 (8055MiB / 11264MiB)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=yalu_model_name,\n",
    "    per_device_train_batch_size=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    # max_steps=2000,\n",
    "    fp16=False,\n",
    "    save_steps=10,\n",
    "    eval_steps=50,\n",
    "    logging_steps=30,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",  # Cosine scheduler with restarts\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=wildlife_train_ds_transformed,\n",
    "    tokenizer=image_processor,\n",
    ")\n",
    "\n",
    "train_results = trainer.train()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='292' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [292/292 01:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     0.1064\n",
      "  eval_runtime            = 0:01:53.13\n",
      "  eval_samples_per_second =     20.622\n",
      "  eval_steps_per_second   =      2.581\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(test_ds)\n",
    "trainer.log_metrics(\"test\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
